{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleRNN, LSTM, Dense\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 2. Generate a synthetic sequential dataset\n",
    "# Assume we have sequences of 20 timesteps and 10 features, for a binary classification task.\n",
    "num_samples = 2000\n",
    "timesteps = 20\n",
    "features = 10\n",
    "\n",
    "# Generate random data and a binary target label (0 or 1)\n",
    "X = np.random.randn(num_samples, timesteps, features)\n",
    "y = np.random.randint(2, size=(num_samples, 1))\n",
    "\n",
    "# 3. Preprocess and normalize the dataset\n",
    "# Flatten the time dimension for scaling and then reshape back\n",
    "X_reshaped = X.reshape(-1, features)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X = X_scaled.reshape(num_samples, timesteps, features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Define a Simple RNN model in Keras\n",
    "def build_simple_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(timesteps, features), activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Also define an LSTM model for comparison\n",
    "def build_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(timesteps, features), activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# 5. Compile and train the models\n",
    "def compile_and_train(model, X_train, y_train, model_name, epochs=15, batch_size=32):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_split=0.2, verbose=1)\n",
    "    return history\n",
    "\n",
    "# Build and train SimpleRNN model\n",
    "simple_rnn_model = build_simple_rnn()\n",
    "history_rnn = compile_and_train(simple_rnn_model, X_train, y_train, \"SimpleRNN\")\n",
    "\n",
    "# Build and train LSTM model\n",
    "lstm_model = build_lstm()\n",
    "history_lstm = compile_and_train(lstm_model, X_train, y_train, \"LSTM\")\n",
    "\n",
    "# 6. Evaluate performance using test data (loss and accuracy)\n",
    "loss_rnn, acc_rnn = simple_rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "loss_lstm, acc_lstm = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"\\nPerformance on Test Data:\")\n",
    "print(f\"SimpleRNN - Loss: {loss_rnn:.4f}, Accuracy: {acc_rnn:.4f}\")\n",
    "print(f\"LSTM      - Loss: {loss_lstm:.4f}, Accuracy: {acc_lstm:.4f}\")\n",
    "\n",
    "# 7. Display training results using plots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation accuracy for SimpleRNN\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_rnn.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_rnn.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('SimpleRNN Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss for SimpleRNN\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_rnn.history['loss'], label='Train Loss')\n",
    "plt.plot(history_rnn.history['val_loss'], label='Val Loss')\n",
    "plt.title('SimpleRNN Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optionally, plot LSTM training curves for comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation accuracy for LSTM\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_lstm.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_lstm.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('LSTM Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss for LSTM\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Val Loss')\n",
    "plt.title('LSTM Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Observations on RNNâ€™s ability to handle sequential data\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- The SimpleRNN model and LSTM model are both designed for sequential data.\")\n",
    "print(\"- SimpleRNN uses a straightforward recurrent connection, which may work well for shorter sequences.\")\n",
    "print(\"- LSTM, with its gating mechanisms, is often better at capturing long-term dependencies.\")\n",
    "print(\"- In this example with synthetic data, both models yield similar performance metrics; however, for more complex sequences, LSTM may perform better.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
